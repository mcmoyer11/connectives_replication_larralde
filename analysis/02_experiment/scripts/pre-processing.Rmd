---
title: "Pre-processing for Replication of Larralde et Noveck (in prep) with Disjunction"
author: Morgan Moyer
date: 22 March, 2023
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00")
```


```{r set wd and read-in data, include=FALSE, warning=FALSE, echo=FALSE}
# Set wd
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../helpers.R")

# User-defined function to read in PCIbex Farm results files
read.pcibex <- function(filepath, auto.colnames=TRUE, fun.col=function(col,cols){cols[cols==col]<-paste(col,"Ibex",sep=".");return(cols)}) {
  n.cols <- max(count.fields(filepath,sep=",",quote=NULL),na.rm=TRUE)
  if (auto.colnames){
    cols <- c()
    con <- file(filepath, "r")
    while ( TRUE ) {
      line <- readLines(con, n = 1, warn=FALSE)
      if ( length(line) == 0) {
        break
      }
      m <- regmatches(line,regexec("^# (\\d+)\\. (.+)\\.$",line))[[1]]
      if (length(m) == 3) {
        index <- as.numeric(m[2])
        value <- m[3]
        if (is.function(fun.col)){
         cols <- fun.col(value,cols)
        }
        cols[index] <- value
        if (index == n.cols){
          break
        }
      }
    }
    close(con)
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=cols))
  }
  else{
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=seq(1:n.cols)))
  }
}

d <- read.pcibex("../data/results.csv")
```

# Initial preprocesing

Make columsn of the correct type.
```{r}
d$ReactionTime1 <- as.numeric(d$ReactionTime1)
d$Connective <- as.factor(d$Connective)
d$ID = as.factor(d$ID)
```

Remove extra space from Connective factor level.

```{r}
# first remove the extra space for some of the connectives

levels(d$Connective)
d$Connective[d$Connective =="so "] = "so"
d$Connective[d$Connective =="and "] = "and"
d$Connective[d$Connective =="filler and "] = "filler and"
d$Connective[d$Connective =="filler but "] = "filler but"

# get rid of the unnecessary levels now
d$Connective <- droplevels(d$Connective)
levels(d$Connective)

```

# Need clean up the file to remove all the extra dummy lines so each line is a single observation

```{r}
length(unique(d$ID)) # 90 subjects because unnecessary columns are being 


# First select only columns that will be important later, then reduce duplicate by first grouping by the 
# unique identifying columnes (TrialID and (Participant) ID)
# Then remove duplicates by calling "unique()"
dd <- d %>%
  # Only filler and test items
  filter(Label == "test") %>% 
  select(ID,Connective,ReactionTime1,ReactionTime2,ReactionTime3,TrialID,Label,Wordform,Answeraccuracy,Truthvalue,Sentenceform,Group) %>% 
  group_by(ID,TrialID) 

length(unique(dd$ID))

d.test <- dd %>% 
  unique()

# Why only 78 subjects?
length(unique(d.test$ID)) # 80 subjects as expected

# Sanity check: the means should be the same even with duplicated rows
mean(d.test$Answeraccuracy) == mean(dd$Answeraccuracy)
mean(d.test$ReactionTime1) == mean(dd$ReactionTime1)

# SD is not the same
sd(d.test$Answeraccuracy) == sd(dd$Answeraccuracy)
sd(d.test$ReactionTime1) == sd(dd$ReactionTime1)

# View(d.test)

nrow(d.test)/nrow(dd)
nrow(d.test)/nrow(d)

length(levels(d.test$ID))
d.test$ID <- droplevels(d.test$ID)
length(levels(d.test$ID))
table(d.test$ID)
```


```{r sanity checking, run=FALSE}
# View(d)
nrow(d$ID) #67440
length(unique(d$ID)) # 90
table(d$ID)

table(d$ID)

# remove random participants
random.IDs <- d %>% 
  filter(ID %in% c(
    "b",
    "0",
    "l",
    "f",
    "NULL",
    "t",
    "Wait success",
    "CapsLock",
    "Shift",
    "e",
    "n",
    "y",
    "s"
   ))

```





# Dummy-code binary 'order' variable (first half, second half)

## Make Binary Order
```{r}
# View(d.test)
# How many rows per participant?
table(d.test$ID)
nrow(d.test)
# how many per half?
108/2

# First create a vector with the two values repeated
binary.order.vector <- c(rep(x = "first", times = 54), rep(x = "second", times = 54))
# It should be possible to just assign the vector to d.test using
# d.test$Order <- as.factor(order.vector)
# but this isn't working, so try first repeating the vector the same number of times as 
# the datafame, but preserving the order in the right way.

repeated.vector <- rep.int(binary.order.vector,times=78)
# should be equal to the length of the d.test
length(repeated.vector) == nrow(d.test)

# then add to the d.test as the column "Order"
d.test$OrderBinary <- repeated.vector

# sanity check
head(table(d.test$ID,d.test$OrderBinary))

```


## Then make an Ordinal Order Variable over Trial Number because this is what Cecile did too
```{r}
trial.order.vector <- rep.int(c(1:108),times=78)
length(trial.order.vector) == nrow(d.test)

d.test$OrderTrials <- trial.order.vector

head(table(d.test$ID,d.test$OrderTrials))
```



# Take a look at training items
```{r, run=FALSE}
d.training <- d %>% 
  filter(Label == c("warmup1","warmup2")) 

# View(d.training)
```


# Take a look at comments and demo info
```{r}
unique(d$PennElementType)
gender <- d %>% 
  group_by(Gender) %>% 
  summarize(count = n())
View(gender)

table(d$Gender)

h <- d %>% 
  group_by(Handedness) %>% 
  summarize(count = n())
View(h)

table(d$Handedness)

table(d$Age)
```

# Look at Overall Accuracy on Fillers + Test items

```{r, graph false response fillers}

summary(d.test$Answeraccuracy)
sd(d.test$Answeraccuracy)

agr <- d.test %>%
  # filter(Label == "test") %>% 
  group_by(Connective) %>%
  summarize(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")

```


## Look at Accuracy on fillers
```{r, graph false response fillers}
agr <- d %>%
  filter(grepl("filler",Connective)) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")

```


## Accuracy on Test Trials
```{r}
agr <- d %>%
  filter(Connective %in% c("and","and ","but","so", "so ")) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")
```


# Removing participants who are not accurate overall

## Look at summary statistcs
```{r}
mean(d.test$Answeraccuracy)*100 #92%
sd(d.test$Answeraccuracy)*100

agr <- d.test %>% 
  # filter(Label == "test") %>% 
  group_by(ID) %>% 
  summarize(IndividualMeanAccuracy = mean(Answeraccuracy))

# View(agr)


d.test %>% 
  filter(ID == "597cdb8ffa99240001dec26e") %>% 
  group_by(Connective) %>% 
  summarise(Mean = mean(Answeraccuracy)) 
```

###  Rabbit hole
Go down a rabbit hole of trying to look at a weird partiticpant who is potentially
inverting their answers....
```{r, run=FALSE}
names(d)
range(d$Results.reception.time)

d$uniqueID <- paste(d$Results.reception.time,d$MD5.hash.of.participant.s.IP.address,sep = '_')

who <- d %>% 
  filter(ID  == "59e1e960d838ae00018507e3") %>% 
  View

# Get a person who is really good
a <- d %>% 
  filter(uniqueID == "1678722549_00773db099f1c891224bc4344925698d") %>% 
  filter(Label == "warmup2")
  
length(a)
range(a$Results.reception.time)


# Get the person who is not very good
t <- d %>% 
  filter(Label == "warmup2") %>%
  filter(uniqueID == "1678721639_14e2e8ec95f85d45bb8657e81a6e6c83")
  # filter(ID == "597cdb8ffa99240001dec26e") %>%
  # View()

range(t$Results.reception.time)
length(t)


range(a$Results.reception.time) - range(t$Results.reception.time)

t <- d %>% 
  filter(Label == "warmup2") %>%
  filter(uniqueID == "1678721639_14e2e8ec95f85d45bb8657e81a6e6c83")

```


## First remove people who are clearly off by visual inspection

```{r}
inacc.below70 <- d.test %>% 
  # filter(Label == "test") %>% 
  group_by(ID) %>% 
  summarize(IndividualMeanAccuracy = mean(Answeraccuracy)) %>% 
  filter(IndividualMeanAccuracy < .7) 

length(unique(inacc.below70$ID))

# View(inacc.below70)

d.test.accurate <- d.test %>% 
  filter(!ID %in% inacc.below70$ID) 

summary(d.test.accurate$Answeraccuracy)

# View(d.test.accurate)
# How much data removed total?
1 - nrow(d.test.accurate)/nrow(d.test)
```

## Second, Remove people 3sd below the mean Answeraccuracy
```{r}
# Trials were counterbalanced in two lists
# table(d.test.accurate$ID,d.test.accurate$TrialID)

# First count the number of correct trails per participant
dc <- d.test.accurate %>% 
  group_by(ID) %>% 
  count(Answeraccuracy, ID)
# View(df)

dc <- dc[dc$Answeraccuracy == "1",]
summary(dc$n)

# View(dc)

mean(dc$n)
sd(dc$n)*3

#Isolating the outliers in a third df 
outliers <- dc[dc$n < mean(dc$n)-(sd(dc$n)*3),]


length(outliers$ID)
head(outliers)

nrow(dc) # 8640

d.test.accurate.outliers.removed <- d.test.accurate %>% 
  filter(!ID %in% outliers$ID)

# how much data removed with this step?
1 - nrow(d.test.accurate.outliers.removed)/nrow(d.test.accurate)

```


# remove unsucessful trials 

```{r}
# Only test items 
d.test.accurate.outliers.removed.nofillers <- d.test.accurate.outliers.removed %>% 
  filter(Connective %in% c("and","but","so"))

# Remove unsuccessful trials
d.test.accurate.sucessful <- d.test.accurate.outliers.removed %>% 
  # Only look at the test trials
  filter(Connective %in% c("and","but","so")) %>%
  group_by(ID,TrialID) %>% 
  filter(Answeraccuracy != "0")

# Percentage of correct responses?
nrow(d.test.accurate.sucessful)/nrow(d.test.accurate.outliers.removed.nofillers)*100
```


# Outlier removal for ReactionTime1

## Look at the summary stats pre-removal of the raw
```{r}
d.test.accurate.sucessful$ReactionTime1 <- as.numeric(d.test.accurate.sucessful$ReactionTime1)

mean(d.test.accurate.sucessful$ReactionTime1)
sd(d.test.accurate.sucessful$ReactionTime1)
summary(d.test.accurate.sucessful$ReactionTime1)

range(d.test.accurate.sucessful$ReactionTime1)

hist(d.test.accurate.sucessful$ReactionTime1,breaks=20, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")
```


## Convert to LogReaction time so that the data is normally distributed
```{r, logRT1 outliers}
# Create the LogRT measure
d.test.accurate.sucessful$LogReactionTime1 <- as.numeric(log(d.test.accurate.sucessful$ReactionTime1))

mean(d.test.accurate.sucessful$LogReactionTime1)
sd(d.test.accurate.sucessful$LogReactionTime1)
summary(d.test.accurate.sucessful$LogReactionTime1)
range(d.test.accurate.sucessful$LogReactionTime1)

hist(d.test.accurate.sucessful$LogReactionTime1,breaks=20, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

```

## cutoff criterion 2.5 * sd of mean 
```{r}

n <- sd(d.test.accurate.sucessful$LogReactionTime1)*2.5 # 28192.71
n

lower <- mean(d.test.accurate.sucessful$LogReactionTime1) - n
upper <- mean(d.test.accurate.sucessful$LogReactionTime1) + n
lower
upper

d.test.accurate.sucessful.no.outliers <- d.test.accurate.sucessful %>% 
  filter(LogReactionTime1 < upper) %>% 
  filter(LogReactionTime1 > lower)

summary(d.test.accurate.sucessful.no.outliers$LogReactionTime1)
sd(d.test.accurate.sucessful.no.outliers$LogReactionTime1)
range(d.test.accurate.sucessful.no.outliers$LogReactionTime1)


hist(d.test.accurate.sucessful.no.outliers$LogReactionTime1, breaks=20, col="red", xlab="RT (log ms)",
        main="Histogram with Normal Curve")

# Finally how much from total raw test data?
1 - nrow(d.test.accurate.sucessful.no.outliers)/nrow(d.test.accurate.sucessful) # almost 1%
```

```{r}
write.csv(d.test.accurate.sucessful.no.outliers,"../data/log_rt1_processed.csv")
```

