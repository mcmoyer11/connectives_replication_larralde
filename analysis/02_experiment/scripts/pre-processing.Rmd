---
title: "Pre-processing for Replication of Larralde et Noveck (in prep) with disjunctions in the training"
author: Morgan Moyer
date: 6 March, 2023
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00")
```


```{r, set wd and read in the data, include=FALSE, warning=FALSE, echo=FALSE}
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../helpers.R")

# User-defined function to read in PCIbex Farm results files
read.pcibex <- function(filepath, auto.colnames=TRUE, fun.col=function(col,cols){cols[cols==col]<-paste(col,"Ibex",sep=".");return(cols)}) {
  n.cols <- max(count.fields(filepath,sep=",",quote=NULL),na.rm=TRUE)
  if (auto.colnames){
    cols <- c()
    con <- file(filepath, "r")
    while ( TRUE ) {
      line <- readLines(con, n = 1, warn=FALSE)
      if ( length(line) == 0) {
        break
      }
      m <- regmatches(line,regexec("^# (\\d+)\\. (.+)\\.$",line))[[1]]
      if (length(m) == 3) {
        index <- as.numeric(m[2])
        value <- m[3]
        if (is.function(fun.col)){
         cols <- fun.col(value,cols)
        }
        cols[index] <- value
        if (index == n.cols){
          break
        }
      }
    }
    close(con)
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=cols))
  }
  else{
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=seq(1:n.cols)))
  }
}

d <- read.pcibex("../data/results.csv")
```



```{r}
d$ID = as.factor(d$ID)

dt <- d %>% 
  filter(Connective %in% c("and","but","so"))

dt$ReactionTime1 <- as.numeric(dt$ReactionTime1)
summary(dt$ReactionTime1)
```

```{r}
# View(d)
nrow(d) #67440

length(unique(d$ID)) # 90
```

Why 90 participants
```{r}
View(table(d$ID))

d[d$ID == "a"]

d <- d %>% 
  filter((substr(ID, 1, 1) == "6") |
           (substr(ID, 1, 1) == "5") |
           (ID == "shift") |
           (ID == "NULL") |
           (ID == "Wait success") |
           (ID == "CapsLock") 
         )

View(table(d$ID))

```


# Take a look at comments and Problems
```{r}
unique(d$PennElementType)
gender <- d %>% 
  group_by(Gender) %>% 
  summarize(count = n())
View(gender)

table(d$Gender)

h <- d %>% 
  group_by(Handedness) %>% 
  summarize(count = n())
View(h)

table(d$Handedness)

table(d$Age)
```



# Accuracy on Fillers

```{r, graph false response fillers}
agr <- d %>%
  filter(grepl("filler",Connective)) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")

```



# Accuracy on Test Trials
```{r}
agr <- d %>%
  filter(Connective %in% c("and","but","so")) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")
```


# Outlier removal

Grand mean on answer accuracy or mean grouped by condition?

## first filter to just the test items
```{r}
d.test <- d %>% 
  filter(Connective %in% c("and","but","so"))

d.test$Answeraccuracy <- as.numeric(d.test$Answeraccuracy)

mean(d.test$Answeraccuracy)
sd(d.test$Answeraccuracy)

# three sd below grand mean
AccuracyCuttoff <- mean(d.test$Answeraccuracy) - 2.5*sd(d.test$Answeraccuracy)
AccuracyCuttoff
```

## Look at participant means to see if there are any really outstading(ly bad)
```{r}
part.means <- d.test %>% 
  group_by(ID) %>% 
  summarize(IndividualMeanAccuracy = mean(Answeraccuracy))

summary(part.means$IndividualMeanAccuracy)

View(part.means)

# look at the lowest values
head(part.means[order(part.means$IndividualMeanAccuracy),])
```

## which sd to use?
```{r}
sd.part.means <- sd(part.means$individualmeanAccuracy)
sd.part.means

sd.grand.means <- sd(d.test$Answeraccuracy)
sd.grand.means

AccuracyCuttoff <- mean(d.test$Answeraccuracy) - 2.5*sd.part.means
AccuracyCuttoff

# filter out participants with accuracy lower than grand mean

inacc <- d.test %>% 
  group_by(ID) %>% 
  summarize(individualmeanAccuracy = mean(Answeraccuracy)) %>% 
  filter(individualmeanAccuracy < AccuracyCuttoff)


length(unique(inacc$ID)) # 4

View(inacc)

```

# remove the inaccurate subjects
```{r}
d.test.clean <- d.test %>% 
  filter(ID != inacc$ID)

View(d.test.clean)

# proportion of test data remaining 
nrow(d.test.clean)/nrow(d.test)
```



# Remove unsucessful trials 

```{r}
d.test.clean.sucessful <- d.test.clean %>% 
  group_by(ID,TrialID) %>% 
  filter(Answeraccuracy != "0")

# proportion of accurate subjects reamining
nrow(d.test.clean.sucessful)/nrow(d.test.clean)
# proportion of total test data
nrow(d.test.clean.sucessful)/nrow(d.test)
```


# Outlier removal based on ReactionTime measures

## RT1
```{r, trialRT outliers}
d.test.clean.sucessful$ReactionTime1 <- as.numeric(d.test.clean.sucessful$ReactionTime1)

summary(d.test.clean.sucessful$ReactionTime1)
   # Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   #    0     634     843    1147    1204   77713 

hist(d.test.clean.sucessful$ReactionTime1, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

# cutoff criterion 2.5 * sd of mean 

part.means.rt1 <- d.test.clean.sucessful %>% 
  group_by(ID) %>% 
  summarize(IndividualRT1Means = mean(ReactionTime1))


OutlierCriterion.GrandSD <- sd(d.test.clean.sucessful$ReactionTime1)*2.5
OutlierCriterion.GrandSD # 5458.681

OutlierCriterion.PartSD <- sd(part.means.rt1$IndividualRT1Means)*2.5 
OutlierCriterion.PartSD # 1478.657

# remove subjects with RT higher than 3 x IQR
df.rt1.clean.sucessful.no.outliers.partsd <- d.test.clean.sucessful %>% 
  filter(ReactionTime1 < OutlierCriterion.PartSD)

hist(df.rt1.clean.sucessful.no.outliers.partsd$ReactionTime1, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

df.rt1.clean.sucessful.no.outliers.grandsd <- d.test.clean.sucessful %>% 
  filter(ReactionTime1 < OutlierCriterion.GrandSD)

hist(df.rt1.clean.sucessful.no.outliers.grandsd$ReactionTime1, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

nrow(df.rt1.clean.sucessful.no.outliers.partsd)/nrow(d.test) # 75%
nrow(df.rt1.clean.sucessful.no.outliers.grandsd)/nrow(d.test) # 88%
```

```{r}
write.csv(df.rt1.clean.sucessful.no.outliers.partsd,"../data/rt1_processed.csv")
```


## RT2
```{r, trialRT outliers}
d$ReactionTime2 <- as.numeric(d$ReactionTime2)

d.test <- d %>% 
  filter(Connective %in% c("and","but","so"))

summary(d.test$ReactionTime2)
   # Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   #   61     760    1002    1180    1391   15797 

hist(d.test$ReactionTime2, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

# cutoff criterion 2.5 * sd of mean 
n <- sd(d.test$ReactionTime2)*2.5 # 1781.288
n
# remove subjects with RT higher than 3 x IQR
df.rt2.clean <- subset(d.test, d.test$ReactionTime2 < n)

hist(df.rt2.clean$ReactionTime2, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

nrow(df.rt2.clean)/nrow(d.test) # 87%
```

```{r}
write.csv(df.rt1.clean,"../data/rt2_processed.csv")
```


