---
title: "Pre-processing for Replication of Larralde et Noveck (in prep)"
author: Morgan Moyer
date: 22 March, 2023
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00")
```


```{r set wd and read-in data, include=FALSE, warning=FALSE, echo=FALSE}
# Set wd
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../helpers.R")

# User-defined function to read in PCIbex Farm results files
read.pcibex <- function(filepath, auto.colnames=TRUE, fun.col=function(col,cols){cols[cols==col]<-paste(col,"Ibex",sep=".");return(cols)}) {
  n.cols <- max(count.fields(filepath,sep=",",quote=NULL),na.rm=TRUE)
  if (auto.colnames){
    cols <- c()
    con <- file(filepath, "r")
    while ( TRUE ) {
      line <- readLines(con, n = 1, warn=FALSE)
      if ( length(line) == 0) {
        break
      }
      m <- regmatches(line,regexec("^# (\\d+)\\. (.+)\\.$",line))[[1]]
      if (length(m) == 3) {
        index <- as.numeric(m[2])
        value <- m[3]
        if (is.function(fun.col)){
         cols <- fun.col(value,cols)
        }
        cols[index] <- value
        if (index == n.cols){
          break
        }
      }
    }
    close(con)
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=cols))
  }
  else{
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=seq(1:n.cols)))
  }
}

d <- read.pcibex("../data/results.csv")
```


```{r sanity checking}
d$ID = as.factor(d$ID)

# View(d)
nrow(d$ID) #67440
length(unique(d$ID))

# View(d)

# first remove the extra space for some of the connectives
d$Connective <- as.factor(d$Connective)
levels(d$Connective)
d$Connective[d$Connective =="so "] = "so"
d$Connective[d$Connective =="and "] = "and"
d$Connective[d$Connective =="filler and "] = "filler and"
d$Connective[d$Connective =="filler but "] = "filler but"
# get rid of the unnecessary levels now
d$Connective <- droplevels(d$Connective)
levels(d$Connective)

```

# Need clean up the file to remove all the extra dummy lines so each line is a single observation

```{r}
View(d)

dd <- d %>%
  filter(Label == "test") %>% 
  select(ID,Connective,ReactionTime1,ReactionTime2,ReactionTime3,TrialID,Label,Wordform,Answeraccuracy,Truthvalue,Sentenceform,Group) %>% 
  group_by(ID,TrialID) 


d.test <- dd %>% 
  unique()

# Sanity check
d.test$ReactionTime1 <- as.numeric(d.test$ReactionTime1)
mean(d.test$ReactionTime1)
sd(d.test$ReactionTime1)

mean(dd$ReactionTime1)
sd(dd$ReactionTime1)



# View(d.test)

nrow(d.test)
nrow(d.test)/nrow(df)
nrow(d.test)/nrow(d)

length(unique(d.test$ID)) # 80 subjects as expected

```



# Dummy-code binary 'order' variable (first half, second half)

```{r}
length(levels(d.test$ID))

# remove random participants
d.test <- d.test %>% 
  filter(! ID %in% c(
    "b",
    "0",
    "l",
    "f",
    "NULL",
    "t",
    "Wait success",
    "CapsLock",
    "Shift",
    "e",
    "n",
    "y",
    "s"
   ))

d.test$ID <- droplevels(d.test$ID)

length(levels(d.test$ID))
```

come back to this, why isn't it working????
```{r}
# View(d.test)
# How many rows per participant?
table(d.test$ID)
nrow(d.test)/108
# how many per half?



# First create a vector with the two values repeated
order.vector <- c(rep(x = "first", times = 54), rep(x = "second", times = 54))
order.vector
# then add to the df
d.test$Order <- order.vector

# d.test$Order2 <- cbind(order.vector) 

# sanity check
table(d.test$ID,d.test$Order)
```

# Take a look at training items
```{r}
d.training <- d %>% 
  filter(Label == c("warmup1","warmup2")) 

# View(d.training)
```


# Take a look at comments and demo info
```{r}
unique(d$PennElementType)
gender <- d %>% 
  group_by(Gender) %>% 
  summarize(count = n())
View(gender)

table(d$Gender)

h <- d %>% 
  group_by(Handedness) %>% 
  summarize(count = n())
View(h)

table(d$Handedness)

table(d$Age)
```

# Look at Overall Accuracy on Fillers + Test items

```{r, graph false response fillers}

summary(d.test$Answeraccuracy)
sd(d.test$Answeraccuracy)

agr <- d.test %>%
  # filter(Label == "test") %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")

```


## Look at Accuracy on fillers
```{r, graph false response fillers}
agr <- d %>%
  filter(grepl("filler",Connective)) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")

```


## Accuracy on Test Trials
```{r}
agr <- d %>%
  filter(Connective %in% c("and","and ","but","so", "so ")) %>% 
  group_by(Connective) %>%
  mutate(mean_accuracy = mean(Answeraccuracy))

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Connective,y=mean_accuracy,fill=Connective)) +
  geom_bar(position=dodge,stat="identity")
```


# Removing participants who are not accurate overall

## Look at summary statistcs
```{r}
mean(d.test$Answeraccuracy)*100 #92%
sd(d.test$Answeraccuracy)*100
```


## First remove people who are clearly off by visual inspection

```{r}
inacc.below70 <- d.test %>% 
  # filter(Label == "test") %>% 
  group_by(ID) %>% 
  summarize(IndividualMeanAccuracy = mean(Answeraccuracy)) %>% 
  filter(IndividualMeanAccuracy < .7) 

# View(inacc.below70)

d.test.accurate <- d.test %>% 
  filter(!ID %in% inacc.below70$ID) 

summary(d.test.accurate$Answeraccuracy)

# View(d.test.accurate)
# How much data removed total?
1 - nrow(d.test.accurate)/nrow(d.test)
```

## Second, Remove people 3sd below the mean answer accuracy
```{r}
# Trials were counterbalanced in two lists
table(d.test.accurate$ID,d.test.accurate$TrialID)

# First count the number of correct trails per participant
dc <- d.test.accurate %>% 
  group_by(ID) %>% 
  count(Answeraccuracy, ID)
# View(df)

dc <- dc[dc$Answeraccuracy == "1",]
summary(dc$n)

View(dc)

mean(dc$n)
sd(dc$n)*3

#Isolating the outliers in a third df 
outliers <- dc[dc$n < mean(dc$n)-(sd(dc$n)*3),]


length(outliers$ID)
head(outliers)

nrow(dc) # 8640

d.test.accurate.outliers.removed <- d.test.accurate %>% 
  filter(!ID == outliers$ID)

# how much data removed with this step?
1 - nrow(d.test.accurate.outliers.removed)/nrow(d.test.accurate)

```


# remove unsucessful trials 

```{r}
# Only test items 
d.test.accurate.outliers.removed.nofillers <- d.test.accurate.outliers.removed %>% 
  filter(Connective %in% c("and","but","so"))

# Remove unsuccessful trials
d.test.accurate.sucessful <- d.test.accurate.outliers.removed %>% 
  # Only look at the test trials
  filter(Connective %in% c("and","but","so")) %>%
  group_by(ID,TrialID) %>% 
  filter(Answeraccuracy != "0")

# Percentage of correct responses?
nrow(d.test.accurate.sucessful)/nrow(d.test.accurate.outliers.removed.nofillers)*100
```


# Outlier removal for ReactionTime1

## Look at the summary stats pre-removal of the raw
```{r}
d.test.accurate.sucessful$ReactionTime1 <- as.numeric(d.test.accurate.sucessful$ReactionTime1)

mean(d.test.accurate.sucessful$ReactionTime1)
sd(d.test.accurate.sucessful$ReactionTime1)
summary(d.test.accurate.sucessful$ReactionTime1)

range(d.test.accurate.sucessful$ReactionTime1)


hist(d.test.accurate.sucessful$ReactionTime1,breaks=20, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")
```


## Convert to LogReaction time so that the data is normally distributed
```{r, logRT1 outliers}
# Create the LogRT measure
d.test.accurate.sucessful$LogReactionTime1 <- as.numeric(log(d.test.accurate.sucessful$ReactionTime1))

mean(d.test.accurate.sucessful$LogReactionTime1)
sd(d.test.accurate.sucessful$LogReactionTime1)
summary(d.test.accurate.sucessful$LogReactionTime1)
range(d.test.accurate.sucessful$LogReactionTime1)

hist(d.test.accurate.sucessful$LogReactionTime1,breaks=20, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

```

## cutoff criterion 2.5 * sd of mean 
```{r}

n <- sd(d.test.accurate.sucessful$LogReactionTime1)*2.5 # 28192.71
n

lower <- mean(d.test.accurate.sucessful$LogReactionTime1) - n
upper <- mean(d.test.accurate.sucessful$LogReactionTime1) + n
lower
upper

d.test.accurate.sucessful.no.outliers <- d.test.accurate.sucessful %>% 
  filter(LogReactionTime1 < upper) %>% 
  filter(LogReactionTime1 > lower)

summary(d.test.accurate.sucessful.no.outliers$LogReactionTime1)
sd(d.test.accurate.sucessful.no.outliers$LogReactionTime1)
range(d.test.accurate.sucessful.no.outliers$LogReactionTime1)


hist(d.test.accurate.sucessful.no.outliers$LogReactionTime1, breaks=20, col="red", xlab="RT (log ms)",
        main="Histogram with Normal Curve")

# Finally how much from total raw test data?
1 - nrow(d.test.accurate.sucessful.no.outliers)/nrow(d.test.accurate.sucessful) # almost 1%
```

```{r}
write.csv(d.test.accurate.sucessful.no.outliers,"../data/log_rt1_processed.csv")
```

